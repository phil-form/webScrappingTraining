{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. Scrapy\n",
    "\n",
    "Objectif : comprendre ce qu'est Scrapy et quand l'utiliser.\n",
    "\n",
    "Scrapy est un framework Python dédié au scraping web. Il fournit :\n",
    "- Une architecture prête à l'emploi (spiders, pipelines, middlewares).\n",
    "- Un moteur asynchrone performant.\n",
    "- Un système de requêtes/réponses simple à tester.\n",
    "\n",
    "Quand l'utiliser :\n",
    "- Pour crawler plusieurs pages avec un vrai suivi de liens.\n",
    "- Pour industrialiser une collecte avec ré-essais et règles propres.\n",
    "- Quand on veut séparer extraction, nettoyage et stockage.\n",
    "\n",
    "Installation (hors notebook) :\n",
    "```\n",
    "pip install scrapy\n",
    "```"
   ],
   "id": "321e2443ee58f2fb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Vérifie l'import de Scrapy (nécessite l'installation préalable).\n",
    "import scrapy\n",
    "\n",
    "# Affiche la version installée pour la traçabilité.\n",
    "print('Scrapy version:', scrapy.__version__)"
   ],
   "id": "a6df589a37ab888e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "2. Introduction aux mécanismes de base du Framework\n",
    "\n",
    "Objectif : parcourir les composants essentiels de Scrapy.\n",
    "\n",
    "Composants principaux :\n",
    "- Spider : définit comment crawler et extraire les données.\n",
    "- Request/Response : encapsulent les requêtes et les réponses HTTP.\n",
    "- Item : structure de données pour les champs extraits.\n",
    "- Pipeline : nettoyage, validation et persistance.\n",
    "\n",
    "Ci-dessous, on crée une Spider minimale qui parse un HTML local.\n",
    "Cela permet d'expliquer le flux sans faire d'appel réseau réel."
   ],
   "id": "246c19d36b72bf28"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from scrapy import Spider\n",
    "from scrapy.http import TextResponse, Request\n",
    "\n",
    "# Définition d'un Item simple (structure des données extraites).\n",
    "class ArticleItem(dict):\n",
    "    # On utilise un dict pour rester simple dans un notebook.\n",
    "    # En projet Scrapy, on utiliserait scrapy.Item + scrapy.Field.\n",
    "    pass\n",
    "\n",
    "# Spider minimaliste qui parse du HTML statique.\n",
    "class DemoSpider(Spider):\n",
    "    # Nom unique de la spider.\n",
    "    name = 'demo'\n",
    "\n",
    "    # Fonction de parsing appelée avec une Response.\n",
    "    def parse(self, response):\n",
    "        # Sélectionne les titres d'articles via un sélecteur CSS.\n",
    "        for title in response.css('h2.article-title::text').getall():\n",
    "            # Nettoie l'espace inutile autour du texte.\n",
    "            cleaned = title.strip()\n",
    "            # Construit l'item et le renvoie.\n",
    "            item = ArticleItem(title=cleaned)\n",
    "            yield item\n",
    "\n",
    "# HTML local pour simuler une page web.\n",
    "html = \"\"\"\n",
    "<html>\n",
    "  <body>\n",
    "    <h2 class=\"article-title\"> Introduction à Scrapy </h2>\n",
    "    <h2 class=\"article-title\"> Crawler des liens proprement </h2>\n",
    "  </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Fabrique un objet Response comme si Scrapy l'avait créé.\n",
    "request = Request(url='https://example.local')\n",
    "response = TextResponse(url='https://example.local', request=request, body=html, encoding='utf-8')\n",
    "\n",
    "# Instancie la spider et exécute parse manuellement.\n",
    "spider = DemoSpider()\n",
    "items = list(spider.parse(response))\n",
    "\n",
    "# Affiche les items extraits.\n",
    "print(items)"
   ],
   "id": "260d5de1f7cea263"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "2.bis. Anatomie d'un projet Scrapy (structure)\n",
    "\n",
    "Objectif : comprendre les dossiers standards créés par scrapy startproject.\n",
    "\n",
    "Structure typique :\n",
    "- spiders/ : contient les spiders.\n",
    "- items.py : définition des champs.\n",
    "- pipelines.py : traitement et stockage.\n",
    "- settings.py : configuration globale.\n",
    "\n",
    "Dans un vrai projet, on lance :\n",
    "```\n",
    "scrapy startproject monprojet\n",
    "scrapy genspider example example.com\n",
    "```"
   ],
   "id": "8bb0f473a44fec8a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "2.ter. Exemple simple de Request et callback\n",
    "\n",
    "Objectif : illustrer le cycle Request → Response → parse.\n",
    "\n",
    "Ici, on ne fait pas d'appel réseau. On simule la réponse reçue."
   ],
   "id": "5baced4f21d54455"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from scrapy.http import TextResponse\n",
    "\n",
    "# URL cible fictive.\n",
    "url = 'https://example.local/articles'\n",
    "\n",
    "# HTML de réponse fictive.\n",
    "html2 = \"\"\"\n",
    "<html>\n",
    "  <body>\n",
    "    <a href=\"/a1\">Article 1</a>\n",
    "    <a href=\"/a2\">Article 2</a>\n",
    "  </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Simule la réponse.\n",
    "response2 = TextResponse(url=url, body=html2, encoding='utf-8')\n",
    "\n",
    "# Extraction de liens par sélecteur CSS.\n",
    "links = response2.css('a::attr(href)').getall()\n",
    "\n",
    "# Affiche les liens extraits.\n",
    "print('Liens:', links)"
   ],
   "id": "63caf6401a568705"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "2.quater. Items et Pipelines (nettoyage et validation)\n",
    "\n",
    "Objectif : montrer comment structurer les données et les nettoyer.\n",
    "\n",
    "Idée :\n",
    "- Un item est une structure de données normalisée.\n",
    "- Un pipeline transforme l'item (nettoyage, filtrage, stockage).\n",
    "\n",
    "Dans un vrai projet, ces classes vivent dans items.py et pipelines.py."
   ],
   "id": "a4d07be44a252451"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Définition d'un item explicite avec des champs connus.\n",
    "class BlogPostItem(dict):\n",
    "    # Ici on reste simple (dict). Scrapy propose scrapy.Item.\n",
    "    pass\n",
    "\n",
    "# Pipeline minimaliste pour nettoyer un titre.\n",
    "class CleanTitlePipeline:\n",
    "    def process_item(self, item, spider):\n",
    "        # Récupère le champ title si présent.\n",
    "        title = item.get('title', '')\n",
    "        # Nettoie les espaces superflus.\n",
    "        item['title'] = title.strip()\n",
    "        # Filtre : si le titre est vide, on lève une erreur.\n",
    "        if not item['title']:\n",
    "            raise ValueError('Titre vide')\n",
    "        # Retourne l'item nettoyé.\n",
    "        return item\n",
    "\n",
    "# Démonstration manuelle du pipeline.\n",
    "raw_item = BlogPostItem(title='  Hello Scrapy  ')\n",
    "pipeline = CleanTitlePipeline()\n",
    "clean_item = pipeline.process_item(raw_item, spider=None)\n",
    "print(clean_item)"
   ],
   "id": "7b68be123dd50b9e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "2.quinquies. Exemple réseau réel (crawler simple)\n",
    "\n",
    "Objectif : exécuter une vraie requête HTTP avec Scrapy.\n",
    "\n",
    "Le spider ci-dessous récupère le titre de https://example.com/.\n",
    "C'est un site public de test, stable et adapté aux démonstrations."
   ],
   "id": "e7981d09c175ae2b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy import Request\n",
    "\n",
    "# Spider minimal pour récupérer le titre d'une page réelle.\n",
    "class ExampleComSpider(Spider):\n",
    "    name = 'example_com'\n",
    "    allowed_domains = ['example.com']\n",
    "    start_urls = ['https://example.com/']\n",
    "\n",
    "    def parse(self, response):\n",
    "        # Extrait le contenu de la balise <title>.\n",
    "        page_title = response.css('title::text').get()\n",
    "        # Nettoie les espaces éventuels.\n",
    "        page_title = (page_title or '').strip()\n",
    "        # Retourne un item simple.\n",
    "        yield {'title': page_title, 'url': response.url}\n",
    "\n",
    "# Configuration minimale pour un run dans un notebook.\n",
    "settings = {\n",
    "    # Empêche Scrapy de créer des fichiers logs trop verbeux.\n",
    "    'LOG_LEVEL': 'ERROR',\n",
    "    # Respecte le robots.txt pour un comportement correct.\n",
    "    'ROBOTSTXT_OBEY': True,\n",
    "}\n",
    "\n",
    "# Lance le crawler (cela peut prendre quelques secondes).\n",
    "process = CrawlerProcess(settings=settings)\n",
    "process.crawl(ExampleComSpider)\n",
    "process.start()"
   ],
   "id": "82d452ce3bbcf4e7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "2.sexies. Suivre des liens et normaliser les donnees\n",
    "\n",
    "Objectif : montrer response.follow et un ItemLoader simple.\n",
    "\n",
    "Idee :\n",
    "- Extraire des liens depuis une page.\n",
    "- Suivre chaque lien avec un callback.\n",
    "- Nettoyer les champs via un loader.\n",
    "\n",
    "Cet exemple reste local et ne lance pas de vrai crawl."
   ],
   "id": "1eead869bc09154"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from scrapy.loader import ItemLoader\n",
    "from itemloaders.processors import TakeFirst, MapCompose\n",
    "\n",
    "# Item explicite avec un loader pour normaliser.\n",
    "class LinkItem(scrapy.Item):\n",
    "    url = scrapy.Field()\n",
    "    label = scrapy.Field()\n",
    "\n",
    "# Loader qui applique des transformations simples.\n",
    "class LinkItemLoader(ItemLoader):\n",
    "    default_output_processor = TakeFirst()\n",
    "    label_in = MapCompose(str.strip)\n",
    "\n",
    "# HTML local avec des liens.\n",
    "html3 = \"\"\"\n",
    "<html>\n",
    "  <body>\n",
    "    <a href=\"/a1\">  Article 1  </a>\n",
    "    <a href=\"/a2\">  Article 2  </a>\n",
    "  </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Simule une reponse.\n",
    "response3 = TextResponse(url='https://example.local', body=html3, encoding='utf-8')\n",
    "\n",
    "# Extrait chaque lien et construit un item.\n",
    "items3 = []\n",
    "for link in response3.css('a'):\n",
    "    loader = LinkItemLoader(item=LinkItem(), selector=link)\n",
    "    loader.add_css('url', '::attr(href)')\n",
    "    loader.add_css('label', '::text')\n",
    "    items3.append(loader.load_item())\n",
    "\n",
    "print(items3)"
   ],
   "id": "12a3f833cdd19984"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "2.septies. Export JSON (idee de pipeline)\n",
    "\n",
    "Objectif : montrer comment ecrire des items en JSON.\n",
    "\n",
    "Dans un vrai projet, on configurerait FEEDS dans settings.py.\n",
    "Ici, on simule l'ecriture avec json pour la pedagogie."
   ],
   "id": "ab14125463c6f305"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import json\n",
    "\n",
    "# Items d'exemple a exporter.\n",
    "sample_items = [\n",
    "    {'title': 'Intro Scrapy', 'url': 'https://example.com/'},\n",
    "    {'title': 'Crawler propre', 'url': 'https://example.com/2'},\n",
    "]\n",
    "\n",
    "# Ecrit un fichier JSON local.\n",
    "with open('scrapy_items.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(sample_items, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print('Export JSON ok:', len(sample_items))"
   ],
   "id": "242a4dbf3102cc3d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "3. Exercices\n",
    "\n",
    "1. Ajouter un champ \"url\" dans l'item renvoye par la spider.\n",
    "2. Extraire un resume depuis un <p> et nettoyer les espaces.\n",
    "3. Modifier la spider pour suivre des liens (yield Request).\n",
    "4. Ajouter un pipeline qui filtre les titres vides.\n",
    "5. Ecrire un spider qui scrape JSONPlaceholder (posts).\n",
    "6. Enregistrer les items dans un fichier JSON.\n",
    "\n",
    "Conseil : commencez par une page HTML locale pour valider vos selecteurs.\n"
   ],
   "id": "49481ca0113a72f1"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
