{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1. Rappel sur le protocole HTTP\n",
    "\n",
    "Objectifs pédagogiques :\n",
    "- Comprendre les principes fondamentaux du protocole HTTP\n",
    "- Maîtriser les méthodes HTTP et les codes de réponse\n",
    "- Savoir construire des requêtes web correctes\n",
    "\n",
    "Concepts clés :\n",
    "- HTTP (HyperText Transfer Protocol): Protocole de communication client-serveur utilisé pour transférer des ressources sur le Web\n",
    "- Client: Votre application (navigateur, script Python, etc.)\n",
    "- Serveur: Ordinateur distant qui fournit les données\n",
    "- Requête HTTP: Demande du client au serveur (méthode, URL, headers, body)\n",
    "- Réponse HTTP: Réponse du serveur au client (code statut, headers, body)\n",
    "\n",
    "Méthodes HTTP courantes :\n",
    "- GET: Récupérer une ressource (données, pages HTML, etc.) - SANS effet de bord\n",
    "- POST: Envoyer des données au serveur pour créer une ressource - AVEC effet de bord\n",
    "- PUT: Remplacer complètement une ressource existante\n",
    "- DELETE: Supprimer une ressource\n",
    "- PATCH: Modifier partiellement une ressource\n",
    "\n",
    "Codes de statut HTTP :\n",
    "* 2xx: Succès\n",
    "  - 200 OK: Requête réussie\n",
    "  - 201 Created: Ressource créée\n",
    "* 3xx: Redirection\n",
    "  - 301 Moved Permanently: Redirection permanente\n",
    "  - 304 Not Modified: Contenu non modifié (cache)\n",
    "* 4xx: Erreur du client\n",
    "  - 400 Bad Request: Requête mal formée\n",
    "  - 401 Unauthorized: Authentification requise\n",
    "  - 404 Not Found: Ressource non trouvée\n",
    "* 5xx: Erreur du serveur\n",
    "  - 500 Internal Server Error: Erreur serveur\n",
    "\n",
    "Structure d'une URL :\n",
    "\n",
    "  https://jsonplaceholder.typicode.com/posts/1\n",
    "\n",
    "     |      |                                 |\n",
    "    Schéma Domaine (host)                   Chemin (path)\n",
    "\n",
    "https://user:pass@example.com:8080/path?key=value#fragment\n",
    "\n",
    "       |          |         |    |        |         |\n",
    "    Credentials   Host    Port  Path   Paramètres  Ancre\n",
    "\n",
    "Exemples d'URL :\n",
    "- https://example.com : Schéma HTTPS, domaine example.com\n",
    "- https://api.example.com/users/123 : API REST, ressource utilisateur\n",
    "- https://example.com/search?q=python : Requête avec paramètres"
   ],
   "id": "6c07aeed90e74b91"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "fed0999ceb5aed25"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2. Requêtes simples avec Requests\n",
    "\n",
    "Objectifs pédagogiques :\n",
    "- Installer et utiliser la bibliothèque Requests\n",
    "- Effectuer des requêtes HTTP GET simples\n",
    "- Interpréter les réponses et les codes de statut\n",
    "\n",
    "Concepts clés :\n",
    "- requests: Bibliothèque Python pour effectuer des requêtes HTTP facilement\n",
    "- Response object: Objet retourné par les requêtes contenant statut, headers, body\n",
    "- JSON API: API retournant des données au format JSON (très courante)\n",
    "- status_code: Code numérique indiquant si la requête a réussi\n",
    "- .json(): Méthode pour parser automatiquement le contenu JSON\n",
    "\n",
    "Workflow typique :\n",
    "1. Importer `requests`\n",
    "2. Construire l'URL\n",
    "3. Effectuer la requête avec `requests.get()` (ou POST, PUT, etc.)\n",
    "4. Vérifier le code de statut\n",
    "5. Parser la réponse (.json(), .text, .content)"
   ],
   "id": "1f2208753bcd42b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T14:18:50.688357666Z",
     "start_time": "2026-02-09T14:18:49.601195834Z"
    }
   },
   "cell_type": "code",
   "source": "%pip install requests",
   "id": "53a1a59da82e328",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests\r\n",
      "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\r\n",
      "Collecting charset_normalizer<4,>=2 (from requests)\r\n",
      "  Downloading charset_normalizer-3.4.4-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (37 kB)\r\n",
      "Collecting idna<4,>=2.5 (from requests)\r\n",
      "  Downloading idna-3.11-py3-none-any.whl.metadata (8.4 kB)\r\n",
      "Collecting urllib3<3,>=1.21.1 (from requests)\r\n",
      "  Using cached urllib3-2.6.3-py3-none-any.whl.metadata (6.9 kB)\r\n",
      "Collecting certifi>=2017.4.17 (from requests)\r\n",
      "  Downloading certifi-2026.1.4-py3-none-any.whl.metadata (2.5 kB)\r\n",
      "Downloading requests-2.32.5-py3-none-any.whl (64 kB)\r\n",
      "Downloading certifi-2026.1.4-py3-none-any.whl (152 kB)\r\n",
      "Downloading charset_normalizer-3.4.4-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (153 kB)\r\n",
      "Downloading idna-3.11-py3-none-any.whl (71 kB)\r\n",
      "Using cached urllib3-2.6.3-py3-none-any.whl (131 kB)\r\n",
      "Installing collected packages: urllib3, idna, charset_normalizer, certifi, requests\r\n",
      "Successfully installed certifi-2026.1.4 charset_normalizer-3.4.4 idna-3.11 requests-2.32.5 urllib3-2.6.3\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m26.0.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-09T14:18:52.833473940Z",
     "start_time": "2026-02-09T14:18:52.696191203Z"
    }
   },
   "source": [
    "import requests\n",
    "\n",
    "# ===== EFFECTUER UNE REQUÊTE GET =====\n",
    "# jsonplaceholder.typicode.com: API publique gratuite pour tester (JSONPlaceholder)\n",
    "# /posts/1: Ressource spécifique (post avec ID 1)\n",
    "url = 'https://jsonplaceholder.typicode.com/posts/1'\n",
    "\n",
    "# requests.get() envoie une requête HTTP GET au serveur\n",
    "# Cette fonction BLOQUE jusqu'à recevoir une réponse du serveur\n",
    "response = requests.get(url)\n",
    "\n",
    "# ===== ANALYSER LA RÉPONSE =====\n",
    "# response.status_code: Code numérique du statut (200 = succès, 404 = non trouvé, etc.)\n",
    "print('Statut de la requête:', response.status_code)\n",
    "\n",
    "# Vérification du succès (optionnelle mais recommandée)\n",
    "if response.status_code == 200:\n",
    "    print('✓ Requête réussie!')\n",
    "else:\n",
    "    print(f'✗ Erreur {response.status_code}')\n",
    "\n",
    "# response.json() parse automatiquement le contenu JSON\n",
    "# Retourne des types Python natifs (dict, list, str, int, bool, None)\n",
    "print('Contenu JSON:', response.json())\n",
    "\n",
    "# ===== ACCÉDER AUX DONNÉES PARSÉES =====\n",
    "# Depuis que response.json() retourne un dictionnaire Python\n",
    "data = response.json()\n",
    "print(f\"\\nID du post: {data['id']}\")\n",
    "print(f\"Titre: {data['title']}\")\n",
    "print(f\"Contenu: {data['body'][:50]}...\")  # Premiers 50 caractères\n",
    "\n",
    "# ===== NOTES IMPORTANTES =====\n",
    "# 1. requests.get() bloque l'exécution en attendant la réponse\n",
    "# 2. TOUJOURS vérifier le status_code avant de traiter les données\n",
    "# 3. response.json() lève une exception si le contenu n'est pas JSON valide\n",
    "# 4. Alternatives à .json():\n",
    "#    - .text: Retourne le contenu brut en string\n",
    "#    - .content: Retourne le contenu brut en bytes\n",
    "#    - .headers: Dictionnaire des headers de réponse\n",
    "# 5. Timeouts: requests.get(url, timeout=5) pour éviter de bloquer indéfiniment\n",
    "# 6. Headers personnalisés: requests.get(url, headers={'User-Agent': 'Mon App'})\n",
    "# 7. Paramètres de requête: requests.get(url, params={'key': 'value'})"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statut de la requête: 200\n",
      "Contenu JSON: {'userId': 1, 'id': 1, 'title': 'sunt aut facere repellat provident occaecati excepturi optio reprehenderit', 'body': 'quia et suscipit\\nsuscipit recusandae consequuntur expedita et cum\\nreprehenderit molestiae ut ut quas totam\\nnostrum rerum est autem sunt rem eveniet architecto'}\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 3. Stocker les données avec SQLAlchemy\n",
    "\n",
    "Objectifs pédagogiques :\n",
    "- Comprendre le concept d'ORM (Object-Relational Mapping)\n",
    "- Maîtriser la création et utilisation de modèles SQLAlchemy\n",
    "- Effectuer les opérations CRUD (Create, Read, Update, Delete) en base de données\n",
    "- Travailler avec SQLite pour un stockage persistant\n",
    "\n",
    "Concepts clés :\n",
    "- ORM (Object-Relational Mapping): Technique pour convertir les objets Python en lignes de base de données\n",
    "- SQLAlchemy: Bibliothèque Python puissante pour interagir avec les bases de données\n",
    "- SQLite: Base de données légère, fichier local (parfait pour apprentissage)\n",
    "- Session: Objet de gestion des transactions avec la base de données\n",
    "- Modèle: Classe Python représentant une table de la base de données\n",
    "- Column: Champ d'une table avec type et propriétés\n",
    "- Primary Key: Identifiant unique pour chaque ligne\n",
    "\n",
    "Types SQLAlchemy courants :\n",
    "- Integer : Nombre entier\n",
    "- String(100) : Texte limité à 100 caractères\n",
    "- Text : Texte long (illimité)\n",
    "- Float : Nombre décimal\n",
    "- Boolean : Vrai/Faux\n",
    "- DateTime : Date et heure\n",
    "\n",
    "Workflow CRUD :\n",
    "1. Create: Créer et insérer une nouvelle ligne\n",
    "2. Read: Lire/récupérer des lignes\n",
    "3. Update: Modifier une ligne existante\n",
    "4. Delete: Supprimer une ligne"
   ],
   "id": "a1f4a51e8597b7ff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T14:19:44.229570182Z",
     "start_time": "2026-02-09T14:19:42.065723944Z"
    }
   },
   "cell_type": "code",
   "source": "%pip install sqlalchemy",
   "id": "24c1c56163687bb7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sqlalchemy\r\n",
      "  Downloading sqlalchemy-2.0.46-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (9.5 kB)\r\n",
      "Collecting greenlet>=1 (from sqlalchemy)\r\n",
      "  Downloading greenlet-3.3.1-cp313-cp313-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.7 kB)\r\n",
      "Collecting typing-extensions>=4.6.0 (from sqlalchemy)\r\n",
      "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\r\n",
      "Downloading sqlalchemy-2.0.46-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (3.3 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.3/3.3 MB\u001B[0m \u001B[31m10.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading greenlet-3.3.1-cp313-cp313-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (612 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m612.9/612.9 kB\u001B[0m \u001B[31m9.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\r\n",
      "Installing collected packages: typing-extensions, greenlet, sqlalchemy\r\n",
      "Successfully installed greenlet-3.3.1 sqlalchemy-2.0.46 typing-extensions-4.15.0\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m26.0.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T14:21:11.360857477Z",
     "start_time": "2026-02-09T14:21:11.315582127Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sqlalchemy import create_engine, Column, Integer, String, Text\n",
    "from sqlalchemy.orm import sessionmaker, declarative_base\n",
    "\n",
    "# ===== CONFIGURATION DE LA BASE DE DONNÉES =====\n",
    "# create_engine() crée une connexion à la base de données\n",
    "# 'sqlite:///web_data2.db' : utilise SQLite, fichier local web_data2.db\n",
    "# echo=False : ne pas afficher les requêtes SQL exécutées (True pour déboguer)\n",
    "Base = declarative_base()\n",
    "engine = create_engine('sqlite:///web_data2.db', echo=False)\n",
    "\n",
    "# Session: Objet pour gérer les transactions (ajouter, modifier, supprimer)\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "# ===== DÉFINIR UN MODÈLE =====\n",
    "# Un modèle est une classe Python représentant une table en base de données\n",
    "# Chaque attribut = une colonne, chaque instance = une ligne\n",
    "class Post(Base):\n",
    "    __tablename__ = 'posts'  # Nom de la table en base de données\n",
    "\n",
    "    # Colonnes avec leurs types et propriétés\n",
    "    id = Column(Integer, primary_key=True)  # Primary key: identifiant unique\n",
    "    title = Column(String(200))              # String limité à 200 caractères\n",
    "    body = Column(Text)                      # Text: contenu long illimité\n",
    "\n",
    "# ===== CRÉER LES TABLES =====\n",
    "# Base.metadata.create_all(engine) crée toutes les tables définies par les modèles\n",
    "# Si les tables existent déjà, ne fait rien\n",
    "Base.metadata.create_all(engine)\n",
    "print(\"✓ Table 'posts' créée (ou déjà existante)\")\n",
    "\n",
    "# ===== INSÉRER DES DONNÉES (CREATE) =====\n",
    "# Récupérer les données JSON de la requête précédente\n",
    "post_data = response.json()\n",
    "\n",
    "# Créer une nouvelle instance de Post\n",
    "# Les valeurs correspondent aux colonnes définies\n",
    "post = Post(\n",
    "    id=post_data['id'],           # Integer\n",
    "    title=post_data['title'],     # String\n",
    "    body=post_data['body']        # Text\n",
    ")\n",
    "\n",
    "# session.add() : Ajouter l'objet à la session (pas encore en base)\n",
    "session.add(post)\n",
    "# session.commit() : Valider la transaction (écrire en base)\n",
    "session.commit()\n",
    "print(\"✓ Post inséré en base de données\")\n",
    "\n",
    "# ===== LIRE LES DONNÉES (READ) =====\n",
    "# session.query() : Construire une requête\n",
    "# Post : Requêter la table/modèle Post\n",
    "# .first() : Récupérer la première ligne (ou None)\n",
    "stored_post = session.query(Post).first()\n",
    "\n",
    "print('\\nPost stocké en base:')\n",
    "print(f'ID: {stored_post.id}, Titre: {stored_post.title}')\n",
    "\n",
    "# ===== NOTES IMPORTANTES =====\n",
    "# 1. Toujours appeler session.commit() après insert/update/delete\n",
    "# 2. session.query() retourne un objet QueryBuilder (peut être modifié)\n",
    "# 3. Autres méthodes de récupération:\n",
    "#    - .all(): Récupérer TOUS les résultats (liste)\n",
    "#    - .first(): Premier résultat (ou None)\n",
    "#    - .filter(): Ajouter une condition WHERE\n",
    "#    - .count(): Nombre de résultats\n",
    "# 4. Fermer la session : session.close()\n",
    "# 5. Alternatives à SQLAlchemy: peewee, tortoise-orm (async)"
   ],
   "id": "215e6985b3fbc04f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Post stocké en base:\n",
      "ID: 1, Title: sunt aut facere repellat provident occaecati excepturi optio reprehenderit\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 4. Analyser du HTML avec BeautifulSoup\n",
    "\n",
    "Objectifs pédagogiques :\n",
    "- Comprendre la structure du HTML et les balises\n",
    "- Maîtriser BeautifulSoup pour extraire des données de pages web\n",
    "- Utiliser les sélecteurs CSS et les navigateurs d'arbre\n",
    "- Développer des techniques de web scraping responsable\n",
    "\n",
    "Concepts clés :\n",
    "- HTML: Format de balisage pour structurer le contenu web\n",
    "- BeautifulSoup: Bibliothèque Python pour parser et analyser du HTML/XML\n",
    "- Parser: Convertit une chaîne HTML en arborescence navigable\n",
    "- Tag: Élément HTML comme `<div>`, `<p>`, `<a>`\n",
    "- Sélecteurs CSS: Méthodes pour cibler des éléments (ex: `.class`, `#id`, `tag > child`)\n",
    "- Web scraping: Extraction automatisée de données de pages web\n",
    "\n",
    "Structure HTML de base :\n",
    "```html\n",
    "<html>\n",
    "  <head>\n",
    "    <title>Titre de la page</title>\n",
    "  </head>\n",
    "  <body>\n",
    "    <div class=\"container\">\n",
    "      <h1 id=\"header\">Titre principal</h1>\n",
    "      <p>Paragraphe</p>\n",
    "      <a href=\"https://example.com\">Lien</a>\n",
    "    </div>\n",
    "  </body>\n",
    "</html>\n",
    "```\n",
    "\n",
    "Sélecteurs CSS courants :\n",
    "- tag : Sélectionner par nom de balise\n",
    "- .class : Sélectionner par classe CSS\n",
    "- #id : Sélectionner par ID unique\n",
    "- parent > child : Enfant direct\n",
    "- ancestor descendant : Tout descendant\n",
    "- tag[attr=\"value\"] : Sélectionner par attribut\n",
    "\n",
    "Éthique du web scraping :\n",
    "1. Respecter les conditions d'utilisation du site\n",
    "2. Vérifier le `robots.txt` et la politique de scraping\n",
    "3. Ne pas surcharger le serveur (délai entre requêtes)\n",
    "4. S'identifier clairement avec un User-Agent\n",
    "5. Préférer les APIs officielles si disponibles"
   ],
   "id": "e1bb7241f92407c6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T14:21:41.466845375Z",
     "start_time": "2026-02-09T14:21:40.727308372Z"
    }
   },
   "cell_type": "code",
   "source": "%pip install beautifulsoup4",
   "id": "51fd4b8a55e888f5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting beautifulsoup4\r\n",
      "  Downloading beautifulsoup4-4.14.3-py3-none-any.whl.metadata (3.8 kB)\r\n",
      "Collecting soupsieve>=1.6.1 (from beautifulsoup4)\r\n",
      "  Downloading soupsieve-2.8.3-py3-none-any.whl.metadata (4.6 kB)\r\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in ./.venv/lib/python3.13/site-packages (from beautifulsoup4) (4.15.0)\r\n",
      "Downloading beautifulsoup4-4.14.3-py3-none-any.whl (107 kB)\r\n",
      "Downloading soupsieve-2.8.3-py3-none-any.whl (37 kB)\r\n",
      "Installing collected packages: soupsieve, beautifulsoup4\r\n",
      "Successfully installed beautifulsoup4-4.14.3 soupsieve-2.8.3\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m26.0.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T14:21:43.228912797Z",
     "start_time": "2026-02-09T14:21:43.178962574Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# ===== CONTENU HTML EXEMPLE =====\n",
    "# HTML brut à analyser (simule le contenu d'une page web)\n",
    "html_content = '''<html>\n",
    "<head>\n",
    "    <title>Exemple de page</title>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>Hello World!</h1>\n",
    "    <div class=\"container\">\n",
    "        <p class=\"intro\">Ceci est une introduction.</p>\n",
    "        <p>Ceci est un paragraphe normal.</p>\n",
    "        <a href=\"https://example.com\">Cliquez ici</a>\n",
    "    </div>\n",
    "</body>\n",
    "</html>'''\n",
    "\n",
    "# ===== CRÉER UN OBJET BEAUTIFULSOUP =====\n",
    "# BeautifulSoup() parse le HTML en arborescence navigable\n",
    "# 'html.parser' : parseur Python natif (simple, efficace)\n",
    "# Alternatives: 'lxml' (rapide), 'html5lib' (stricte)\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# ===== EXTRACTION SIMPLE =====\n",
    "# Accéder directement à une balise par son nom\n",
    "# soup.title retourne le premier élément <title>\n",
    "print('\\n=== EXTRACTION SIMPLE ===')\n",
    "print('Titre HTML:', soup.title.text)  # .text récupère le contenu texte\n",
    "\n",
    "# soup.h1 retourne le premier <h1>\n",
    "print('Texte H1:', soup.h1.text)\n",
    "\n",
    "# ===== EXTRACTION AVEC FIND() =====\n",
    "# .find('tag') retourne le PREMIER élément avec ce tag\n",
    "print('\\n=== RECHERCHE AVEC FIND() ===')\n",
    "\n",
    "# Trouver le premier paragraphe\n",
    "first_p = soup.find('p')\n",
    "print('Premier <p>:', first_p.text)\n",
    "\n",
    "# Trouver le premier lien\n",
    "link = soup.find('a')\n",
    "# .get('attr') ou ['attr'] pour accéder aux attributs HTML\n",
    "print('Attribut href du lien:', link.get('href'))\n",
    "\n",
    "# ===== EXTRACTION AVEC FIND_ALL() =====\n",
    "# .find_all('tag') retourne une LISTE de tous les éléments\n",
    "print('\\n=== RECHERCHE AVEC FIND_ALL() ===')\n",
    "\n",
    "# Trouver TOUS les paragraphes\n",
    "all_p = soup.find_all('p')\n",
    "print(f'Nombre de <p>: {len(all_p)}')\n",
    "for i, p in enumerate(all_p, 1):\n",
    "    print(f'  P{i}: {p.text}')\n",
    "\n",
    "# ===== SÉLECTION PAR CLASSE CSS =====\n",
    "# .find('tag', class_='classe') ou soup.find_all('tag', class_='classe')\n",
    "print('\\n=== SÉLECTION PAR CLASSE ===')\n",
    "container = soup.find('div', class_='container')\n",
    "print('Contenu du container:', container.text.strip()[:50] + '...')\n",
    "\n",
    "# Trouver éléments avec classe spécifique\n",
    "intro = soup.find('p', class_='intro')\n",
    "print('Élément avec classe \"intro\":', intro.text)\n",
    "\n",
    "# ===== NAVIGATION DANS L'ARBORESCENCE =====\n",
    "print('\\n=== NAVIGATION ARBORESCENTE ===')\n",
    "\n",
    "# .parent : parent de l'élément\n",
    "p_tag = soup.find('p')\n",
    "parent = p_tag.parent\n",
    "print(f'Parent de <p>: <{parent.name}>')  # .name : nom de la balise\n",
    "\n",
    "# .children : enfants (itérateur)\n",
    "# .contents : enfants (liste)\n",
    "div = soup.find('div')\n",
    "print(f'Nombre d\\'enfants du <div>: {len(list(div.children))}')\n",
    "\n",
    "# ===== FILTRER AVEC ATTRIBUTS =====\n",
    "print('\\n=== FILTRAGE PAR ATTRIBUT ===')\n",
    "\n",
    "# Trouver un élément avec un attribut spécifique\n",
    "# attrs est un dictionnaire d'attributs\n",
    "link_with_href = soup.find('a', attrs={'href': 'https://example.com'})\n",
    "print(f'Lien trouvé: {link_with_href.text}')\n",
    "\n",
    "# ===== NOTES IMPORTANTES =====\n",
    "# 1. .find() retourne le PREMIER résultat (ou None)\n",
    "# 2. .find_all() retourne une LISTE (peut être vide)\n",
    "# 3. .text ou .get_text() : obtenir le contenu texte\n",
    "# 4. .name : obtenir le nom de la balise\n",
    "# 5. .attrs ou ['attr'] : accéder aux attributs HTML\n",
    "# 6. .parent, .children, .contents, .next_sibling, .previous_sibling\n",
    "# 7. Sélecteurs CSS avancés: soup.select('.class > p') (nécessite lxml)\n",
    "# 8. Données structurées: utiliser .find_all() + boucles for\n",
    "# 9. Gestion des erreurs: vérifier que l'élément existe avant d'accéder\n",
    "# 10. Performance: BeautifulSoup est lent pour gros volumes, utiliser lxml ou selenium\n",
    "\n"
   ],
   "id": "5f53b86aef2873be",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Titre HTML: Exemple\n",
      "Texte H1: Hello World!\n"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
